{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd55d73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to create tool using @tool decorator\n",
    "#simplest way to define custom tool,decorator uses function name as the toool name by default,\n",
    "#but this can be overridden by passing string as the first argument, \n",
    "#decorator will use the function's docstring as tool's description - so docstring must provided\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def divison(a: int,b: int)->int:\n",
    "    \"\"\"Divide 2 numbers\"\"\"\n",
    "    return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fddb984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divison\n",
      "Divide 2 numbers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(divison.name)\n",
    "print(divison.description)\n",
    "print(divison.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ba1829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#async implementation\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "async def multiply(a: int,b: int)->int:\n",
    "    \"\"\"Multiplies 2 numbers\"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8261ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,List\n",
    "\n",
    "@tool\n",
    "def multiply_by_max(\n",
    "    a:Annotated[int,\"A value\"],\n",
    "    b:Annotated[List[int],\"B value\"]\n",
    ")->int:\n",
    "    \"\"\"Multiply a by the maximum of b\"\"\"\n",
    "    return a*max(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4240292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': {'description': 'A value', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'B value', 'items': {'type': 'integer'}, 'title': 'B', 'type': 'array'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply_by_max.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfd2580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Structured tool\n",
    "#it gives a bit more configurability than the @tool decorator, without requiring much additional code\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "def mutliply(a:int,b:int)->int:\n",
    "    \"\"\"Mutliply 2 numbers\"\"\"\n",
    "    return a*b\n",
    "\n",
    "async def amutliply(a:int,b:int)->int:\n",
    "    \"\"\"Mutliply 2 numbers asynchronusly\"\"\"\n",
    "    return a*b\n",
    "\n",
    "calculator=StructuredTool.from_function(func=mutliply,coroutine=amutliply)\n",
    "#in coroutine gives async function\n",
    "\n",
    "print(calculator.invoke({\"a\":3,\"b\":2}))\n",
    "print(await calculator.ainvoke({\"a\":5,\"b\":4}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1f4e28",
   "metadata": {},
   "source": [
    "Langchain inbuilt tools\n",
    "https://docs.langchain.com/oss/javascript/integrations/tools\n",
    "\n",
    "https://docs.langchain.com/oss/python/integrations/retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e983d9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inbuilt tools\n",
    "\n",
    "#wikipedia\n",
    "\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=500)\n",
    "\n",
    "wiki_tool=WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "print(wiki_tool.invoke({\"query\":\"langchain\"}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d194d3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published: 2023-08-02\\nTitle: Attention Is All You Need\\nAuthors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\\nSummary: The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.utilities import ArxivAPIWrapper\n",
    "\n",
    "tool=ArxivAPIWrapper()\n",
    "\n",
    "tool.run(\"1706.03762\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d78b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a:int,b:int)->int:\n",
    "    \"\"\"Adds 2 numbers\"\"\"\n",
    "    return a+b\n",
    "\n",
    "@tool\n",
    "def multiply(a:int,b:int)->int:\n",
    "    \"\"\"multiplys 2 numbers\"\"\"\n",
    "    return a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ccb99b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\Vansh Parmar\\\\OneDrive\\\\Documents\\\\Generative_AI\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=500)),\n",
       " StructuredTool(name='add', description='Adds 2 numbers', args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x000002512E9A9580>),\n",
       " StructuredTool(name='multiply', description='multiplys 2 numbers', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x000002512E9A8D60>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#call tool with llm model\n",
    "tools=[wiki_tool,add,multiply]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073362ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000002512D97EC30>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002512ED2C6B0>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm=init_chat_model(\"qwen/qwen3-32b\",model_provider=\"groq\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9780413e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000002512D97EC30>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002512ED2C6B0>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'add', 'description': 'Adds 2 numbers', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'multiply', 'description': 'multiplys 2 numbers', 'parameters': {'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools=llm.bind_tools(tools)\n",
    "llm_with_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6517c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'Okay, the user is asking \"What is 2*3\". Let me see. I need to figure out which tool to use here. The available tools are Wikipedia, add, and multiply. The question is a multiplication problem, so the multiply function should handle it. The parameters required for multiply are a and b, both integers. Here, a is 2 and b is 3. So I should call the multiply function with these values. No need to use Wikipedia since it\\'s a straightforward math problem. Let me make sure there\\'s no trick here. 2 multiplied by 3 is 6. Yep, that\\'s simple. So the correct tool call is multiply with arguments 2 and 3.\\n', 'tool_calls': [{'id': '3w9gn58mj', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 174, 'prompt_tokens': 324, 'total_tokens': 498, 'completion_time': 0.287131159, 'completion_tokens_details': {'reasoning_tokens': 145}, 'prompt_time': 0.014189325, 'prompt_tokens_details': None, 'queue_time': 0.158747855, 'total_time': 0.301320484}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b50d8-cd02-73d2-9eea-df009b8146c6-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': '3w9gn58mj', 'type': 'tool_call'}] usage_metadata={'input_tokens': 324, 'output_tokens': 174, 'total_tokens': 498, 'output_token_details': {'reasoning': 145}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "query=\"What is 2*3\"\n",
    "messages=[HumanMessage(query)]\n",
    "response=llm_with_tools.invoke(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5159d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': '3w9gn58mj',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8851edbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 2*3', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content='6', name='multiply', tool_call_id='3w9gn58mj')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for tool_call in response.tool_calls:\n",
    "    selected_tool={\"add\":add,\"multiply\":multiply,\"wikipedia\":wiki_tool}[tool_call[\"name\"].lower()]\n",
    "    tool_msg=selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ff89720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The result of multiplying 2 and 3 is **6**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"What is 2*3\". I need to figure out the right tool to use here. Looking at the available functions, there\\'s a multiply function that takes two integers. The parameters are a and b, both integers. So, 2 and 3 are the numbers to multiply. I should call the multiply function with a=2 and b=3. The result is 6, which is the answer the user is looking for. Let me make sure there\\'s no confusion with the add function. No, the user specifically used the multiplication symbol (*), so multiply is correct. Alright, the tool call was made correctly, and the response was 6. That\\'s all there is to it.\\n'}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 334, 'total_tokens': 501, 'completion_time': 0.312368142, 'completion_tokens_details': {'reasoning_tokens': 148}, 'prompt_time': 0.016349966, 'prompt_tokens_details': None, 'queue_time': 0.057071664, 'total_time': 0.328718108}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b50d8-f1e9-76e0-a36c-f915838e72d7-0', usage_metadata={'input_tokens': 334, 'output_tokens': 167, 'total_tokens': 501, 'output_token_details': {'reasoning': 148}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6baae885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking two things here. First, they want to know what LangChain is. Second, they\\'re asking for the product of 15 multiplied by 15. Let me break this down.\\n\\nFor the first part, \"What is LangChain?\" I need to use the Wikipedia tool to get an accurate description. LangChain is likely a term related to technology or programming, maybe a framework or library. Using the Wikipedia function will help fetch the latest and most reliable information about it.\\n\\nThen, the second part is a math problem: 15 times 15. The user provided a function called \"multiply\" which takes two integers. I should call this function with a=15 and b=15 to get the result. \\n\\nWait, but the user also mentioned \"id 15*15\". Maybe \"id\" here is a typo or shorthand. Since the main query is about multiplying 15 by 15, I\\'ll proceed with the multiply function. \\n\\nSo, I need to make two separate tool calls. One to Wikipedia for LangChain and another to the multiply function. I should structure the response with both tool calls in XML tags as specified.\\n', 'tool_calls': [{'id': 'ed60dh88m', 'function': {'arguments': '{\"query\":\"LangChain\"}', 'name': 'wikipedia'}, 'type': 'function'}, {'id': 'aqvbtx2x5', 'function': {'arguments': '{\"a\":15,\"b\":15}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 298, 'prompt_tokens': 331, 'total_tokens': 629, 'completion_time': 0.832199806, 'completion_tokens_details': {'reasoning_tokens': 246}, 'prompt_time': 0.014183375, 'prompt_tokens_details': None, 'queue_time': 0.159586245, 'total_time': 0.846383181}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b50f5-ea2f-7ef2-9ff6-ab9d8dd23b2e-0', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'LangChain'}, 'id': 'ed60dh88m', 'type': 'tool_call'}, {'name': 'multiply', 'args': {'a': 15, 'b': 15}, 'id': 'aqvbtx2x5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 331, 'output_tokens': 298, 'total_tokens': 629, 'output_token_details': {'reasoning': 246}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query=\"What is langchain and what id 15*15\"\n",
    "messages=[HumanMessage(query)]\n",
    "\n",
    "ai_msg=llm_with_tools.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b8efc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'wikipedia',\n",
       "  'args': {'query': 'LangChain'},\n",
       "  'id': 'ed60dh88m',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'multiply',\n",
       "  'args': {'a': 15, 'b': 15},\n",
       "  'id': 'aqvbtx2x5',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4db3f63e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is langchain and what id 15*15', additional_kwargs={}, response_metadata={}),\n",
       " ToolMessage(content=\"Page: LangChain\\nSummary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\", name='wikipedia', tool_call_id='ed60dh88m'),\n",
       " ToolMessage(content='225', name='multiply', tool_call_id='aqvbtx2x5')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool={\"add\":add,\"multiply\":multiply,\"wikipedia\":wiki_tool}[tool_call[\"name\"].lower()]\n",
    "    tool_msg=selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "524b4862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a software framework that helps integrate large language models (LLMs) into applications, with use cases like document analysis, chatbots, and code analysis.\\n\\nThe result of 15 multiplied by 15 is 225.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=llm_with_tools.invoke(messages)\n",
    "res.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Generative_AI (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
